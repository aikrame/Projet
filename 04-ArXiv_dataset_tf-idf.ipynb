{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some useful functions for computing the *tf-idf* for the data with arXiv abstracts.\n",
    "\n",
    "Note here the use of specific pandas function like *concat*, *value_counts* and *groupby* which make possible to speed up the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./arxiv_articles.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>arxiv_primary_category</th>\n",
       "      <th>summary</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http://arxiv.org/abs/2001.05867v1</td>\n",
       "      <td>$σ$-Lacunary actions of Polish groups</td>\n",
       "      <td>Jan Grebik</td>\n",
       "      <td>math.LO</td>\n",
       "      <td>We show that every essentially countable orbit...</td>\n",
       "      <td>2020-01-16T15:09:02Z</td>\n",
       "      <td>2020-01-16T15:09:02Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://arxiv.org/abs/1303.6933v1</td>\n",
       "      <td>Hans Grauert (1930-2011)</td>\n",
       "      <td>Alan Huckleberry</td>\n",
       "      <td>math.HO</td>\n",
       "      <td>Hans Grauert died in September of 2011. This a...</td>\n",
       "      <td>2013-03-27T19:23:57Z</td>\n",
       "      <td>2013-03-27T19:23:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>http://arxiv.org/abs/1407.3775v1</td>\n",
       "      <td>A New Proof of Stirling's Formula</td>\n",
       "      <td>Thorsten Neuschel</td>\n",
       "      <td>math.HO</td>\n",
       "      <td>A new simple proof of Stirling's formula via t...</td>\n",
       "      <td>2014-07-10T11:26:39Z</td>\n",
       "      <td>2014-07-10T11:26:39Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http://arxiv.org/abs/math/0307381v3</td>\n",
       "      <td>On Dequantization of Fedosov's Deformation Qua...</td>\n",
       "      <td>Alexander V. Karabegov</td>\n",
       "      <td>math.QA</td>\n",
       "      <td>To each natural deformation quantization on a ...</td>\n",
       "      <td>2003-07-30T06:20:33Z</td>\n",
       "      <td>2003-09-20T01:29:18Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http://arxiv.org/abs/1604.06794v1</td>\n",
       "      <td>Cyclic extensions are radical</td>\n",
       "      <td>Mariano Suárez-Álvarez</td>\n",
       "      <td>math.HO</td>\n",
       "      <td>We show that finite Galois extensions with cyc...</td>\n",
       "      <td>2016-04-21T22:24:54Z</td>\n",
       "      <td>2016-04-21T22:24:54Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>http://arxiv.org/abs/1712.09576v2</td>\n",
       "      <td>The Second Main Theorem in the hyperbolic case</td>\n",
       "      <td>Min Ru;Nessim Sibony</td>\n",
       "      <td>math.CV</td>\n",
       "      <td>We develop Nevanlinna's theory for a class of ...</td>\n",
       "      <td>2017-12-27T13:17:08Z</td>\n",
       "      <td>2019-01-03T07:51:11Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  \\\n",
       "0    http://arxiv.org/abs/2001.05867v1   \n",
       "1     http://arxiv.org/abs/1303.6933v1   \n",
       "2     http://arxiv.org/abs/1407.3775v1   \n",
       "3  http://arxiv.org/abs/math/0307381v3   \n",
       "4    http://arxiv.org/abs/1604.06794v1   \n",
       "5    http://arxiv.org/abs/1712.09576v2   \n",
       "\n",
       "                                               title                 authors  \\\n",
       "0              $σ$-Lacunary actions of Polish groups              Jan Grebik   \n",
       "1                           Hans Grauert (1930-2011)        Alan Huckleberry   \n",
       "2                  A New Proof of Stirling's Formula       Thorsten Neuschel   \n",
       "3  On Dequantization of Fedosov's Deformation Qua...  Alexander V. Karabegov   \n",
       "4                      Cyclic extensions are radical  Mariano Suárez-Álvarez   \n",
       "5     The Second Main Theorem in the hyperbolic case    Min Ru;Nessim Sibony   \n",
       "\n",
       "  arxiv_primary_category                                            summary  \\\n",
       "0                math.LO  We show that every essentially countable orbit...   \n",
       "1                math.HO  Hans Grauert died in September of 2011. This a...   \n",
       "2                math.HO  A new simple proof of Stirling's formula via t...   \n",
       "3                math.QA  To each natural deformation quantization on a ...   \n",
       "4                math.HO  We show that finite Galois extensions with cyc...   \n",
       "5                math.CV  We develop Nevanlinna's theory for a class of ...   \n",
       "\n",
       "              published               updated  \n",
       "0  2020-01-16T15:09:02Z  2020-01-16T15:09:02Z  \n",
       "1  2013-03-27T19:23:57Z  2013-03-27T19:23:57Z  \n",
       "2  2014-07-10T11:26:39Z  2014-07-10T11:26:39Z  \n",
       "3  2003-07-30T06:20:33Z  2003-09-20T01:29:18Z  \n",
       "4  2016-04-21T22:24:54Z  2016-04-21T22:24:54Z  \n",
       "5  2017-12-27T13:17:08Z  2019-01-03T07:51:11Z  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naif_regex_tokenize(text):\n",
    "    \"\"\"\n",
    "    This is a very naif way of tokenize a text. Just using the\n",
    "    regular expression \"[a-z]\" that will match any single word\n",
    "    in lowercase.\n",
    "    Returns a list with all the tokens.\n",
    "    \"\"\"\n",
    "    p = re.compile(\"[a-z]+\")\n",
    "    return p.findall(text.lower())\n",
    "\n",
    "def compute_tf(d):\n",
    "    \"\"\"\n",
    "    Compute the tf for a given document d.\n",
    "    The formula used is \n",
    "    \n",
    "        tf(t, d) = 0.5 + 0.5 * (count(t, d)/max(count(t',d) for t' in d))\n",
    "    \n",
    "    This prevents bias in longer documents.\n",
    "    \"\"\"\n",
    "    terms = pd.Series(naif_regex_tokenize(d))\n",
    "    term_counts = terms.value_counts()\n",
    "    max_tc = max(term_counts)\n",
    "    return 0.5 + 0.5 * (term_counts / max_tc)\n",
    "\n",
    "def compute_idf(D):\n",
    "    \"\"\"\n",
    "    The input D is a list of pandas.Series\n",
    "    having as each element, the term frequency \n",
    "    computed by the function compute_tf.\n",
    "    \"\"\"\n",
    "    N = len(D)\n",
    "    all_terms = pd.concat(D)\n",
    "    nt = all_terms.index.value_counts() # The number of documents containing the term \"t\"\n",
    "    return np.log(N / nt)\n",
    "\n",
    "def compute_tf_idf_document(tf_document, idf):\n",
    "    \"\"\"Compute the tf-idf for each term in a document of the corpus\n",
    "\n",
    "    Keyword arguments:\n",
    "    tf_document -- list with the frequency of each term inside the document\n",
    "    idf -- the idf value for each term in the corpus\n",
    "    \"\"\"\n",
    "    return tf_document * np.array([idf[i] for i in tf_document.index])\n",
    "    \n",
    "def compute_tf_idf_corpus(D):\n",
    "    \"\"\"Compute the tf-idf for each term in a corpus\n",
    "\n",
    "    Keyword arguments:\n",
    "    D -- pandas Series containing a collection of documents in text format\n",
    "    \n",
    "    returns\n",
    "        list of pandas Series containing the tf-idf(t, d, D) for each term\n",
    "        inside each document of the corpus D\n",
    "    \"\"\"\n",
    "    term_freq = [compute_tf(d) for d in D]\n",
    "    idf = compute_idf(term_freq)\n",
    "    return [compute_tf_idf_document(d, idf) for d in term_freq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We show that every essentially countable orbit equivalence relation induced by a continuous action of a Polish group on a Polish space is $\\sigma$-lacunary. In combination with [Invent. Math.201 (1), 309-383, 2015] we obtain a straightforward proof of the result from [Adv. Math.307, 312-343,2017] that every essentially countable equivalence relation that is induced by an action of abelian non-archimedean Polish group is essentially hyperfinite.\n"
     ]
    }
   ],
   "source": [
    "s = data['summary'][0]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = data['summary'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = compute_tf_idf_corpus(data.loc[:, \"summary\"]) # Sauvegarder avec pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we have the tf-idf values at each document. In order to select the *most important* terms (i.e. the terms with higher tf-idf values), we compute the **mean** of the tf-idf for each term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "his                4.659259\n",
       "in                 0.121669\n",
       "hans               7.447268\n",
       "grauert            7.967129\n",
       "of                 0.018285\n",
       "accomplishments    6.507696\n",
       "died               5.887687\n",
       "detail             3.147262\n",
       "september          4.729013\n",
       "life               2.903803\n",
       "some               1.532848\n",
       "reviews            4.234578\n",
       "and                0.061446\n",
       "this               0.350511\n",
       "recalls            6.319210\n",
       "major              2.690017\n",
       "mathematics        3.935670\n",
       "article            2.477727\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hans Grauert died in September of 2011. This article reviews his life in mathematics and recalls some detail his major accomplishments.\n"
     ]
    }
   ],
   "source": [
    "print(data[\"summary\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terms = pd.concat(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tf_idf = all_terms.groupby(all_terms.index).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tf_idf = mean_tf_idf.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "karatsuba             10.622838\n",
       "erdi                  10.622838\n",
       "sagetex               10.622838\n",
       "bioelectrodynamics    10.622838\n",
       "koras                 10.622838\n",
       "                        ...    \n",
       "verschiebung           7.967129\n",
       "verses                 7.967129\n",
       "carlisle               7.967129\n",
       "cotter                 7.967129\n",
       "cotranslational        7.967129\n",
       "Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_tf_idf[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.967128798944532"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_terms[\"cotter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique[\"cotter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   \"the\" \"a\" \"model\"\n",
    "d1    0.1 5.8 9\n",
    "d2    4.7 1.0 3\n",
    "d3    8.0 2.4 6.0\n",
    "d4    0.3 9.1 3.2\n",
    "\n",
    "k = 2\n",
    "groupe 1    groupe 2\n",
    "(d1, d4, d3)  (d2)\n",
    "\n",
    "K = 5\n",
    "\n",
    "(d1, d50)     (d2, d3, d10)      (d9, d8)     (d6, d7)    (d20, d32, ...)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = [0.1, 5.8, 9]\n",
    "norme2(x) = S\n",
    "\n",
    "S = sqrt(0.1**2 + 5.8**2 + 9**2)\n",
    "x_normalise = [0.1, 5.8, 9] / S\n",
    "norme2(x_normalise) = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant c'est à vous\n",
    "\n",
    "Utiliser le code et les fonctions ci-dessus pour :\n",
    "\n",
    "1. Proposer un dictionnaire des termes (une liste de mots qui peuvent être \"informatives\"). Par exemple, un mot qui est présent dans plus de 10 documents différents et qui a un fort valeur de *tf-idf* peut etre \"informatif\". \n",
    "\n",
    "2. Utiliser ces termes pour calculer, pour chaque document, un vecteur *tf-idf*. Ce vecteur aura les valeurs du tf-idf de chaqu'un des termes. Par example, si la liste de termes est [\"float\", \"genetic\", \"circular\"] et qu'on a 4 document. On doit produire une matrice de 4 lignes et 3 colonnes :\n",
    "```\n",
    "0.1 5.8 9\n",
    "4.7 1.0 3\n",
    "8.0 2.4 6.0\n",
    "0.3 9.1 3.2\n",
    "```   \n",
    "Ici, chaque ligne contient les valeurs de *tf-idf* de [\"float\", \"genetic\", \"circular\"] (dans cet ordre).\n",
    "\n",
    "3. Normaliser les lignes de cette matrice. La norme 2 des vecteurs *tf-idf* représentés à chaque ligne doit être 1. Les étapes **2** et **3** font partie de ce qu'on appelle *feature extraction*. \n",
    "\n",
    "4. Executer l'exemple décrit [ici](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). Appliquer la même analyse sur notre jeu des données.\n",
    "\n",
    "5. Faire une implémentation de l'algorithme k-means. Voir les liens : https://en.wikipedia.org/wiki/K-means_clustering, https://en.wikipedia.org/wiki/K-means%2B%2B, https://fr.wikipedia.org/wiki/K-moyennes\n",
    "\n",
    "6. Implémenter une fonction permettant de trier les documents en fonction du résultat de l'algorithme k-means avec **k** groupes.\n",
    "\n",
    "7. Executer l'exemple décrit [ici](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py) puis appliquer la même analyse sur notre jeu des données\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
